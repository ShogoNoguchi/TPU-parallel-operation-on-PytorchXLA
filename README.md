# TPU-parallel-operation-on-PytorchXLA
I realized parallel operation of TPU with pytorchXLA. The basic code is the same as the official pytorch tutorial. However, in order to run it in parallel, I reconstructed it while looking at the official XLA tutorial.
