# TPU-parallel-operation-on-PytorchXLA
I realized parallel operation codes of TPU with pytorchXLA. The basic code is the same as the official pytorch tutorial. However, in order to run it in parallel, I reconstructed it while looking at the official XLA tutorial.


Today, I could only use one core because TPU was popular. But the code should be correct, so I'll try it again.

Comments are welcome.
