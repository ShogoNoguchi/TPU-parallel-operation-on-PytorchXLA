# TPU-parallel-operation-on-PytorchXLA
I realized parallel operation codes of TPU with pytorchXLA. The basic code is the same as the official pytorch tutorial. However, in order to run it in parallel, I reconstructed it while looking at the official XLA tutorial.


Today, I could only use one core because TPU was popular. But the code should be correct, so I'll try it again.

Learning can be done according to the number of cores available at the time.

Comments are welcome.

https://pytorch.org/xla/release/r2.5/index.html
https://pytorch.org/
